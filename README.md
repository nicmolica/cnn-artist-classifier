# Artist Classifier - Convolutional Neural Network
This repo contains the data and code for a convolutional neural network model to identify an artist/composer by their song. I also included some data processing scripts I wrote to convert the audio into a good format for a neural network.

## Development Process
This project was developed in collaboration with a classmate, Joshua Kwok (joshua167). I was responsible for data processing, Joshua was responsible for collecting research on CNN improvements, and we shared the task of collecting the initial dataset. We were inspired by Leland Roberts' genre classification project, so we decided to change it a bit and try artist classification. We adapted our neural network code from his (see model.py for attribution), but wrote our data processing code from scratch.

## Data Collection
Because of our limited access to parallel compute devices and the difficulty of obtaining raw audio files for modern music through legitimate sources, we limited the amount number of artists our model supports and the amount of audio per artists. We obtained roughly 60 minutes of audio from 15 artists/composers: Bach, Bill Evans, BTS, Chet Baker, Chopin, Gregory Alan Isakov, John Coltrain, Mamamoo, Mandolin Orange, McCoy Tyner, Mozart, Mumford and Sons, Seventeen, Tchaikovsky, and The Steeldrivers. We wanted to ensure that our model was genuinely identifying artists and not just genres, so we selected multiple artists within multiple genres.

## Data Processing
We wanted our model to be able to identify features in a similar way to how a human might. Humans experience pitch on a logarithmic scale, which western musicians have represented using the 12-tone equal temperament scale that approximates ratios between frequencies, rather than mapping linearly to increasing frequency. Humans also experience loudness on a logarthmic scale, where we hear a 10dB increase as a 2x increase, rather than the 10x increase in intensity it actually is. In order to help our model see as many human-recognizable features of the music as possible, we used mel spectrograms. In order to produce these, I wrote a multithreaded preprocessing script (process_normalized_audio.py) that takes a folder of songs and splits them into 30-second interleaving segments, performs fourier transforms on the segments, plots them as mel spectrograms, and then crops the spectrograms so that only the actual data is included in the image. There is a second script included that converts them to black and white, since color is not necessary to see all the features of a mel spectrogram.

## Our Model
The core functionality of our model is to use convolutions to identify features in the spectrograms. The flattened result of two 2D convolutions (coupled with a max pooling layer and an average pooling layer) is fed through a densely-connected layer of 64 neurons. We used a dropout layer to help with overfitting and then connected this to an output layer of 15 neurons. In its current state, the model achieves about 73% test accuracy on a good run, though there is some variation depending on how the train/test sets are partitioned. We've found that test performance doesn't tend to get better after about 30 epochs, though we're still in the process of improving the model to reduce overfitting and do better than 73%. The results.png file illustrates what a good run looks like at 20 epochs.